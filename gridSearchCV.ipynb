{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850a2c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  {'alpha': 0.1}\n",
      "Best score:  0.607159939336843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV # For Hyperparameter tuning and cross validation\n",
    "from sklearn.linear_model import Ridge # Ridge regression model\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define the hyperparameter gris we want to search over\n",
    "# 'alpha' controls the regularisation strength in Ridge Regression\n",
    "# Smaller alpha = less regularisation (more flexible model)\n",
    "# Larger alpha = more regularisation (simpler model)\n",
    "params = {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "# Set up the Ridge regression model and wrap it in GridSearchCV\n",
    "# This will try each value of alpha using a 5-fold cross-valdation\n",
    "grid = GridSearchCV(Ridge(), param_grid=params, cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "# For each alpha, it runs 5-fold CV and scores the results\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Output the best hyperparameter value found\n",
    "# This tells us which alpha led to the best performance (on advage across the five folds)\n",
    "print(\"Best alpha: \", grid.best_params_)\n",
    "\n",
    "# Output the average cross-validation score for the best model\n",
    "# This is typically Rsquared (default scoring for Ridge in regression)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75cff9",
   "metadata": {},
   "source": [
    "### üî¢ **1. \"Should we do a confusion matrix after GridSearchCV?\"**\n",
    "\n",
    "**When doing classification (e.g. Logistic Regression):**\n",
    "\n",
    "Yes, definitely on the test set. GridSearchCV helps us find the best model, but once we‚Äôve selected it, we still want to evaluate its real-world performance. A confusion matrix shows exactly how your model is making mistakes: which classes are being confused.\n",
    "\n",
    "\n",
    "‚û°Ô∏è **Use:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **2. \"Do we evaluate using classification_report or accuracy?\"**\n",
    "\n",
    "Both are useful‚Äîbut classification_report is better.\n",
    "It gives you **precision, recall, F1-score**, and support for each class, not just overall accuracy. For unbalanced datasets, F1 is a better judge of performance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è **3. \"What‚Äôs the difference between `alpha` in Ridge and `C` in LogisticRegression?\"**\n",
    "\n",
    "They're both regularization parameters‚Äîbut inverse of each other:\n",
    "\n",
    "- `alpha` (Ridge) ‚Üí **higher = stronger regularization**\n",
    "- `C` (LogisticRegression) ‚Üí **lower = stronger regularization**\n",
    "\n",
    "üí° So, `alpha ‚Üë` behaves like `C ‚Üì`.\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ **4. \"Why not use LinearRegression instead of Ridge?\"**\n",
    "\n",
    "You can, but Ridge is more stable, especially when:\n",
    "- You have **many correlated features**\n",
    "- You want to **avoid large, unstable coefficients**\n",
    "\n",
    "It‚Äôs a regularised version that helps prevent **overfitting**, which LinearRegression doesn‚Äôt do.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **5. \"Should we use the test set inside GridSearchCV?\"**\n",
    "\n",
    "No. Never. \n",
    "GridSearchCV only works with the **training set**, splitting it into folds internally.\n",
    "\n",
    "\n",
    "You should **only test on the test set once**, at the end, to get a fair estimate of final performance.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **6. \"Should we use cross_val_score manually instead of GridSearchCV?\"**\n",
    "\n",
    "You could, but GridSearchCV is better for tuning because:\n",
    "\n",
    "- It does the cross-validation **and** searches over parameter combinations automatically\n",
    "- It simplifies the process\n",
    "- Use `cross_val_score()` only if you want to **evaluate a fixed model**, not tune it\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **7. \"What‚Äôs the difference between parameters and hyperparameters?\"**\n",
    "\n",
    "\n",
    "- **Parameters** = learned by the model during training (e.g. weights, intercepts)\n",
    "- **Hyperparameters** = set *before* training, like `C`, `alpha`, `max_depth`\n",
    "\n",
    "GridSearchCV tunes **hyperparameters**, not model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ **8. \"Should we use a pipeline with GridSearchCV?\"**\n",
    "\n",
    "Ideally, yes‚Äîespecially if you‚Äôre scaling, doing PCA, or encoding. \n",
    "\n",
    "A `Pipeline` ensures all transformations (like scaling) are:\n",
    "\n",
    "- Done in the right order\n",
    "- Applied properly within each CV fold (no leakage!)\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **9. \"Is GridSearchCV overkill for small datasets?\"**\n",
    "\n",
    "Not really. Even on small datasets, hyperparameters can make a big difference.\n",
    "\n",
    "But if your data is *very* small, use **fewer folds** (like `cv=3`) or just test manually.\n",
    "\n",
    "---\n",
    "\n",
    "### üïµÔ∏è‚Äç‚ôÇÔ∏è **10. \"How do I know if my model is overfitting or underfitting?\"**\n",
    "\n",
    "Overfitting: Very high train score, low test scoreUnderfitting: Both train and test scores are low\n",
    "\n",
    "Check the **cross-val score vs test score**:\n",
    "\n",
    "- If they match closely, the model generalizes well\n",
    "- If they‚Äôre far apart, it's probably overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
